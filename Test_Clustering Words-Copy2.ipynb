{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4a74da",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb10ec",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd20a7fa",
   "metadata": {},
   "source": [
    "We want to create a program that clusters words into groups based on how similar they are and identifies an 'exemplar' or 'keyword' that represents the group. The goal is to use the program to identify inclusion and exclusion criteria when writing a review and classify useful aricles to read by processing abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0f98d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List : Articles_Google_In (2017-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fb8f20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert excel table to list of strings\n",
    "import pandas\n",
    "pandas.__version__\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "61aa0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a column to list of strings\n",
    "def getExcelString(path, column1) :\n",
    "    agoogleIn = pd.read_excel(path)\n",
    "    agoogleInLst = agoogleIn[column1].tolist()\n",
    "    return agoogleInLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c3251177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSVString(path, column1) :\n",
    "    agoogleIn = pd.read_csv(path)\n",
    "    agoogleInLst = agoogleIn[column1].tolist()\n",
    "    return agoogleInLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "eb6d67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates keywords based on # of records.\n",
    "# NO PROCESSING\n",
    "def duplicateKeyword(path, column1, column2) :\n",
    "    # Grabs list of strings\n",
    "    keywordLst = getExcelString(path, column1)\n",
    "    \n",
    "    # Grabs list of occurances for each keyword\n",
    "    keywordCount = getExcelString(path, column2)\n",
    "    \n",
    "    # Duplicates keywords based on occurances\n",
    "    listDup = list()\n",
    "\n",
    "    for i in range(0, len(keywordLst)) :\n",
    "        for j in range(0, keywordCount[i]):\n",
    "            listDup.append(keywordLst[i])\n",
    "        \n",
    "    # converts listDup back to dataframe\n",
    "    listDupdf = pd.DataFrame(listDup)\n",
    "    \n",
    "    return listDupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd42c30",
   "metadata": {},
   "source": [
    "#### Import modules and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "38dd53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already installed\n",
    "# pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fddb007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already installed\n",
    "# pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3b94cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63c7cf",
   "metadata": {},
   "source": [
    "#### Preliminary text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "66f19859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalized version of the split function\n",
    "# Splits a long string based on any input string : for exampl \",\"\n",
    "\n",
    "def splitCar(array, string) : \n",
    "    \n",
    "    # import module\n",
    "    import re\n",
    "    regex = re.compile(string)\n",
    "    lstTemp = []\n",
    "\n",
    "    for i in range(0,len(array)):\n",
    "        if(regex.search(array[i]) == None):\n",
    "            lstTemp.append(array[i])\n",
    "        else:\n",
    "            lst = array[i].split(string)\n",
    "            lstTemp.extend(lst)\n",
    "            \n",
    "    array = np.array(lstTemp)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1119a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes any asteryx in a string\n",
    "def cutCar(array, string) : \n",
    "    lstTemp = []\n",
    "\n",
    "    for i in range(0,len(array)):\n",
    "        lst = array[i].replace(string, \"\")\n",
    "        lstTemp.append(lst)\n",
    "            \n",
    "    array = np.array(lstTemp)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a22ad6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing excel file AND removes DUPLICATES\n",
    "# IMP : input is a DataFrame; output is list\n",
    "\n",
    "def preprocessing(array) :\n",
    "\n",
    "    # Therefore important to cast all objects to str\n",
    "    array = array.astype(str)\n",
    "\n",
    "    # convert everything to lower case\n",
    "    array = array.str.lower()\n",
    "\n",
    "    # Get all the unique keywords and put it into an array\n",
    "    keywords = array.unique()\n",
    "\n",
    "    # split keywords seperated by '/'\n",
    "    keywords = splitCar(keywords, '/')\n",
    "\n",
    "    # split keywords seperated by '&'\n",
    "    keywords = splitCar(keywords, '&')\n",
    "\n",
    "    # split keywords seperated by ','\n",
    "    keywords = splitCar(keywords, ',')\n",
    "\n",
    "    # clean up asteryx\n",
    "    keywords = cutCar(keywords, \"*\")\n",
    "\n",
    "    # convert the array back into dataframe in order to strip white space\n",
    "    keywordsdf = pd.DataFrame(keywords)\n",
    "\n",
    "    # remove white spaces again\n",
    "    keywordsdf[0] = keywordsdf[0].str.strip()\n",
    "\n",
    "    # converts df back to np.array\n",
    "    # keywords = keywordsdf[0].to_numpy()\n",
    "\n",
    "    # removes duplicates again and puts them back into usable array\n",
    "    keywords = keywordsdf[0].unique()\n",
    "\n",
    "    # sorts alphabatically\n",
    "    keywords.sort()\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "64a913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    #strings = df[column]\n",
    "\n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, limit = 10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "\n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af5f7d",
   "metadata": {},
   "source": [
    "# String similarity Fuzzy Matching + Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b53bf9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already installed\n",
    "# conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "648dedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already installed\n",
    "# pip install Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8d3f49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a27d4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMP This can't take a list with duplicates !\n",
    "def levCluster(keywords) :\n",
    "\n",
    "    lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in keywords] for w2 in keywords])\n",
    "\n",
    "    affprop = AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "    affprop.fit(lev_similarity)\n",
    "\n",
    "    # keywordsUnique = keywords.unique()\n",
    "    tblCluster = []\n",
    "    for cluster_id in np.unique(affprop.labels_):\n",
    "        exemplar = keywords[affprop.cluster_centers_indices_[cluster_id]]\n",
    "        cluster = np.unique(keywords[np.nonzero(affprop.labels_== cluster_id)])\n",
    "\n",
    "\n",
    "        # print(exemplar)\n",
    "        cluster = cluster.tolist()\n",
    "        cluster.insert(0, exemplar)\n",
    "        tblCluster.append(cluster)\n",
    "\n",
    "    return tblCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efc0a6",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c551a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets list of keywords WITHOUT duplicates\n",
    "\n",
    "def preprocessingDup(array) : \n",
    "\n",
    "    # Therefore important to cast all objects to str\n",
    "    array = array.astype(str)\n",
    "\n",
    "    # convert everything to lower case\n",
    "    array = array.str.lower()\n",
    "\n",
    "    # Get all the unique keywords and put it into an array\n",
    "    # keywords = agoogleIn['A'].unique()\n",
    "    keywordsDup = array.to_numpy()\n",
    "\n",
    "    # split keywords seperated by '/'\n",
    "    keywordsDup = splitCar(keywordsDup, '/')\n",
    "\n",
    "    # split keywords seperated by '&'\n",
    "    keywordsDup = splitCar(keywordsDup, '&')\n",
    "\n",
    "    # split keywords seperated by ','\n",
    "    keywordsDup = splitCar(keywordsDup, ',')\n",
    "\n",
    "    # clean up asteryx\n",
    "    keywordsDup = cutCar(keywordsDup, \"*\")\n",
    "\n",
    "    # convert the array back into dataframe in order to strip white space\n",
    "    keywordsDupdf = pd.DataFrame(keywordsDup)\n",
    "\n",
    "    # remove white spaces again\n",
    "    keywordsDupdf[0] = keywordsDupdf[0].str.strip()\n",
    "\n",
    "    # converts df back to np.array\n",
    "    keywordsDup = keywordsDupdf[0].to_numpy()\n",
    "\n",
    "    # sorts alphabatically\n",
    "    keywordsDup.sort()\n",
    "    \n",
    "    return keywordsDup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d2e44da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all duplicates and puts them in a list\n",
    "def getDupes(lst) :\n",
    "    tbl = []\n",
    "    for word in lst :\n",
    "        i = 0\n",
    "\n",
    "        for word_ in lst :\n",
    "            # allows appending duplicates only\n",
    "            if word_ == word :\n",
    "                i += 1\n",
    "\n",
    "            if i == 2 :\n",
    "                tbl.append(word)\n",
    "                i += 1\n",
    "                break\n",
    "    \n",
    "    # return a list of duplicate keywords\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "81395f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the duplicates to the cluster table\n",
    "\n",
    "def addDupes(tblCluster, keywordsDup) :\n",
    "    dupes = getDupes(keywordsDup)\n",
    "    for d in dupes :\n",
    "\n",
    "        for i in range(0, len(tblCluster)) :\n",
    "            for j in tblCluster[i] :\n",
    "                if d == j :\n",
    "                    tblCluster[i].append(d)\n",
    "                    break\n",
    "    return tblCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "35d5d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of keywords under each exemplar and returns list of list of exemplar to count\n",
    "\n",
    "def countKeywords(tblCluster, keywordsDup) :\n",
    "    tblCluster1 = addDupes(tblCluster, keywordsDup)\n",
    "    \n",
    "    tblKeywords = []\n",
    "    for x in tblCluster1 :\n",
    "        tblx = []\n",
    "        count = len(x) - 1\n",
    "        tblx.append(x[0]) \n",
    "        tblx.append(count)\n",
    "        x = tblx\n",
    "        tblKeywords.append(x)\n",
    "\n",
    "    return tblKeywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95e054",
   "metadata": {},
   "source": [
    "### Final program : Lev distance-based Keyword Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "532321e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(path, column1, column2) :\n",
    "    array = duplicateKeywords(path, column1, column2)\n",
    "    \n",
    "    keywords = preprocessing(array[0])\n",
    "    tblCluster = levCluster(keywords)\n",
    "    \n",
    "    keywordsDup = preprocessingDup(array[0])\n",
    "    tblCluster = addDupes(tblCluster, keywordsDup)\n",
    "    \n",
    "    tblKeywords = countKeywords(tblCluster, keywordsDup)\n",
    "    \n",
    "    pd.DataFrame(tblKeywords).to_excel('outputTest.xlsx', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6d872",
   "metadata": {},
   "source": [
    "###### improuvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f198af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering based on semantics rather than lev distance\n",
    "# Output the list with the duplicates in the same file as well so that there is posSibility for modification\n",
    "# dont forget that it didnt expert the # of records from endnote, so find a way to export this as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
